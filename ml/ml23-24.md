
## Question 1(a):

**What is the difference between regression and classification? Explain with example.**

### Answer:

### Difference between Regression and Classification:

| Aspect          | Regression                                  | Classification                                    |
|-----------------|---------------------------------------------|---------------------------------------------------|
| Output Type     | Predicts a **continuous** numerical value   | Predicts a **discrete** label or category         |
| Purpose         | Estimate relationships among variables      | Categorize input into predefined classes          |
| Algorithms Example | Linear Regression, Polynomial Regression    | Decision Trees, Support Vector Machines (SVM)      |
| Output Examples | Salary prediction, House price estimation   | Email spam detection, Disease diagnosis           |
| Evaluation Metrics | Mean Squared Error (MSE), Root Mean Squared Error (RMSE) | Accuracy, Precision, Recall, F1-Score             |

----------

### Example:

-   **Regression Example:**
    Predicting the **price of a house** based on features like area, number of rooms, and location.

-   **Classification Example:**
    Classifying an **email as spam or not spam** based on keywords, sender, and subject.

----------

## Question 1(b):

**Explain the concept of linear regression in detail.**

### Answer:

### Concept of Linear Regression:

-   **Definition:**
    Linear Regression is a **supervised learning algorithm** that models the relationship between a **dependent variable** (target) and one or more **independent variables** (features) by fitting a linear equation.

-   **Equation of Simple Linear Regression:**

    $y = \beta_0 + \beta_1 x + \varepsilon$

    Where:

    -   $y$ = dependent variable (target)

    -   $x$ = independent variable (feature)

    -   $\beta_0$ = intercept

    -   $\beta_1$ = slope (coefficient)

    -   $\varepsilon$ = error term

-   **Objective:**
    To find the best-fitting line (regression line) that minimizes the error between the predicted and actual values.

-   **Cost Function:**
    Mean Squared Error (MSE) is commonly used to measure error:

    $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

    where:

    -   $y_i$ = actual value

    -   $\hat{y}_i$ = predicted value

    -   $n$ = number of data points

-   **Process:**

    1.  Assume a linear relationship between input and output.

    2.  Use **Least Squares Method** to find optimal parameters $\beta_0$ and $\beta_1$.

    3.  Predict future outcomes using the learned model.

----------

### Diagram:

(Imagine a scatter plot where a straight line best fits all points.)

```
|
|   * *
|       * *
|   * * *
|________________________

```

Straight line = predicted regression line.

----------

### Applications:

-   Predicting stock prices

-   Estimating sales revenue

-   Forecasting temperatures

----------
Perfect. Let's continue.

----------

# Question 1 (OR)

## (a)

**Differentiate between supervised, unsupervised, and reinforcement learning with help of suitable examples.**

### Answer:

### Types of Machine Learning:

| Aspect              | Supervised Learning                      | Unsupervised Learning                    | Reinforcement Learning                       |
|---------------------|------------------------------------------|------------------------------------------|--------------------------------------------|
| Definition          | Model learns from labeled data           | Model learns from unlabeled data         | Model learns by interacting with environment |
| Input Data          | Labeled (Input-Output pairs)             | Unlabeled (Input only)                   | Actions and Rewards                          |
| Goal                | Predict output for new inputs            | Find hidden patterns or structure        | Maximize cumulative reward over time        |
| Example Algorithms  | Linear Regression, Decision Trees, SVM   | K-means Clustering, PCA                  | Q-Learning, Deep Q Networks (DQN)          |
| Examples            | Email spam detection, House price prediction | Customer segmentation, Market basket analysis | Self-driving cars, Game playing agents     |

----------

### Simple Examples:

-   **Supervised Learning:**
    Predicting whether an email is spam or not (label: spam or not spam).

-   **Unsupervised Learning:**
    Grouping customers based on their buying habits (no labels).

-   **Reinforcement Learning:**
    Training a robot to walk through trial-and-error interactions.

----------

✅ **Thus, each learning type serves different kinds of real-world problems.**

----------

## (b)

**What do you mean by Machine Learning? How is it different from traditional programming?**

### Answer:

### Machine Learning:

-   **Definition:**
    Machine Learning (ML) is a subfield of Artificial Intelligence where systems **learn from data** to make predictions or decisions without being explicitly programmed for every task.

-   **Arthur Samuel's Definition:**

    > "Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed."

----------

### Difference between Machine Learning and Traditional Programming:

| Aspect              | Traditional Programming             | Machine Learning                          |
|---------------------|-------------------------------------|-------------------------------------------|
| Approach            | Manual creation of rules and logic  | Data-driven learning of patterns and rules |
| Data                | Used for input only                 | Used for both training and testing        |
| Adaptability        | Hard to adapt to new situations     | Learns and adapts to new patterns         |
| Output              | Fixed program output                | Predictive model                          |
| Examples            | Calculator, Payroll systems         | Recommendation systems, Fraud detection    |

----------

### Diagram (Conceptual):

```
Traditional Programming:
Rules + Data => Output

Machine Learning:
Data + Output => Learn Rules (Model)

```

----------

✅ **Machine learning is thus more flexible, powerful, and scalable in complex scenarios.**

----------

## Question 2(a):

**What do you understand by an inductive bias? Briefly explain the types of inductive bias.**

### Answer:

### Inductive Bias:

-   **Definition:**
    Inductive bias refers to the **set of assumptions** a learning algorithm uses to predict outputs for **unseen inputs**.
    Without inductive bias, a machine learning model cannot generalize beyond the training data.

-   **Importance:**

    -   Helps **generalize** from limited training examples.

    -   Guides the learning algorithm to prefer some hypotheses over others.

----------

### Types of Inductive Bias:

1.  **Language Bias:**

    -   Restricts the form of the hypotheses.

    -   Example: Decision trees can only express hypotheses that can be represented as tree structures.

2.  **Preference Bias (Search Bias):**

    -   Prefers one hypothesis over another based on some **criterion**, even if both fit the training data.

    -   Example: Choosing a simpler model (Occam’s Razor principle).

3.  **Algorithmic Bias:**

    -   Inherent in the way algorithms are designed.

    -   Example: K-Nearest Neighbors assumes that similar instances are nearby in feature space.

4.  **Representative Bias:**

    -   Assumes the training examples are representative of the overall population.

----------

## Question 2(b):

**Describe k-means algorithm with the help of an example.**

### Answer:

### K-Means Algorithm:

-   **Definition:**
    K-Means is an **unsupervised clustering algorithm** that partitions a set of data points into **k clusters** where each point belongs to the cluster with the nearest mean.

-   **Objective:**
    Minimize the **intra-cluster variance** (sum of squared distances between points and their respective cluster centroid).

----------

### Steps of K-Means:

1.  **Initialize:**

    -   Choose $k$ initial centroids randomly.

2.  **Assignment Step:**

    -   Assign each data point to the nearest centroid based on Euclidean distance.

3.  **Update Step:**

    -   Calculate the new centroids as the mean of the assigned points.

4.  **Repeat:**

    -   Repeat the assignment and update steps until the centroids do not change significantly.

----------

### Formula:

-   **Distance Metric:**

    $d(x_i, \mu_j) = \sqrt{ \sum_{l=1}^{n} (x_{il} - \mu_{jl})^2 }$

    Where:

    -   $x_i$ = data point

    -   $\mu_j$ = centroid of cluster $j$

----------

### Example:

Suppose we have 6 points: (2, 3), (3, 3), (6, 8), (7, 7), (8, 8), (1, 2) and $k=2$.

-   Step 1: Randomly select 2 initial centroids, say (2, 3) and (6, 8).

-   Step 2: Assign points to the nearest centroid:

    -   Points (2,3), (3,3), (1,2) to centroid (2,3)

    -   Points (6,8), (7,7), (8,8) to centroid (6,8)

-   Step 3: Update centroids:

    -   New centroid 1: Mean of (2,3), (3,3), (1,2) ≈ (2, 2.67)

    -   New centroid 2: Mean of (6,8), (7,7), (8,8) ≈ (7, 7.67)

-   Step 4: Reassign and repeat until convergence.

----------

### Diagram:

(Visualize two groups of points with circles around their new centroids.)

----------

✅ **Thus, K-Means efficiently clusters similar data points together without supervision.**


----------

# Question 2 (OR)

## (a)

**Define support vector, hyperplane and margin and support vector machine.**

### Answer:

### 1. Support Vector:

-   **Definition:**
    Support vectors are the **data points** that are **closest to the separating hyperplane**.

-   **Importance:**
    These points **influence** the position and orientation of the hyperplane. Removing them would change the decision boundary.

----------

### 2. Hyperplane:

-   **Definition:**
    A hyperplane is a **decision boundary** that separates data points of different classes.

-   **In 2D space:** a line

-   **In 3D space:** a plane

-   **In higher dimensions:** a hyperplane.

-   **Mathematical Equation of a Hyperplane:**

    $w \cdot x + b = 0$

    where:

    -   $w$ = weight vector (normal to hyperplane)

    -   $x$ = input features

    -   $b$ = bias term

----------

### 3. Margin:

-   **Definition:**
    Margin is the **distance** between the hyperplane and the **nearest data points** from either class.

-   **Objective in SVM:**
    Maximize this margin to improve model generalization.

----------

### 4. Support Vector Machine (SVM):

-   **Definition:**
    SVM is a **supervised machine learning algorithm** used for **classification** (and sometimes regression) tasks.

-   **Working Principle:**

    -   Find the optimal hyperplane that **maximizes the margin** between different classes.

-   **Advantages:**

    -   Effective in high-dimensional spaces.

    -   Works well with clear margin of separation.

----------

### Diagram (Simple Illustration):

```
Class A:    o o o
                --- Hyperplane ---
Class B:    x x x

```

_(Support vectors are the closest 'o' and 'x' to the hyperplane.)_

----------

✅ **Thus, SVM builds a powerful classifier based on the concept of support vectors and margins.**

----------

## (b)

**What is logistic regression? Differentiate between logistic regression and linear regression.**

### Answer:

### Logistic Regression:

-   **Definition:**
    Logistic Regression is a **supervised classification algorithm** used to **predict binary outcomes** (0 or 1, true or false, spam or not spam).

-   **Key Idea:**
    It uses a **logistic (sigmoid) function** to map predicted values between **0 and 1**.

-   **Sigmoid Function:**

    $\sigma(z) = \frac{1}{1 + e^{-z}}$

    Where:

    -   $z$ is the linear combination of input features.

----------

### Difference between Logistic Regression and Linear Regression:

| Aspect              | Linear Regression                     | Logistic Regression                     |
|---------------------|---------------------------------------|-----------------------------------------|
| Purpose             | Predicts **continuous numerical values** | Predicts **categorical binary outcomes** |
| Output              | Real numbers (-∞ to ∞)                | Probabilities (0 to 1)                  |
| Algorithm Type      | Regression                            | Classification                          |
| Function Used       | Straight line (linear function)       | Sigmoid function                        |
| Example             | Predicting house prices               | Predicting if a customer will churn     |

----------

✅ **Thus, logistic regression is ideal for classification tasks, while linear regression is suited for predicting continuous values.**

----------


## Question 3(a):

**Explain the different evaluation parameters used in classification and regression models.**

### Answer:

### Evaluation Parameters:

#### 1. For **Classification Models**:

-   **Accuracy:**

    -   Measures the proportion of correctly classified instances out of total instances.

    -   Formula:

        $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$
    -   Where:

        -   TP = True Positive

        -   TN = True Negative

        -   FP = False Positive

        -   FN = False Negative

-   **Precision:**

    -   Measures the accuracy of positive predictions.

    -   Formula:

        $Precision = \frac{TP}{TP + FP}$
-   **Recall (Sensitivity or True Positive Rate):**

    -   Measures the ability of a model to find all relevant cases.

    -   Formula:

        $Recall = \frac{TP}{TP + FN}$
-   **F1-Score:**

    -   Harmonic mean of Precision and Recall.

    -   Formula:

        $F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$
-   **ROC-AUC Score:**

    -   Area Under the Receiver Operating Characteristic Curve, indicates model performance across thresholds.

----------

#### 2. For **Regression Models**:

-   **Mean Absolute Error (MAE):**

    -   Average of absolute differences between predicted and actual values.

    -   Formula:

        $MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$
-   **Mean Squared Error (MSE):**

    -   Average of squared differences between predicted and actual values.

    -   Formula:

        $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
-   **Root Mean Squared Error (RMSE):**

    -   Square root of MSE; penalizes large errors more.

    -   Formula:

        $RMSE = \sqrt{MSE}$
-   **R-squared ($R^2$) Score:**

    -   Proportion of variance in dependent variable that is predictable from independent variables.

    -   Formula:

        $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$
    -   Where:

        -   $SS_{res}$ = Residual Sum of Squares

        -   $SS_{tot}$ = Total Sum of Squares

----------

✅ **Thus, different parameters are chosen based on whether the task is classification or regression.**

----------

## Question 3(b):

**What is ensemble learning? What is the difference between bagging and boosting?**

### Answer:

### Ensemble Learning:

-   **Definition:**
    Ensemble learning is a technique that combines **multiple models** (usually called **weak learners**) to produce a **stronger model** with better predictive performance.

-   **Purpose:**

    -   To reduce variance (overfitting) and bias (underfitting).

    -   Improve model accuracy and robustness.

----------

### Bagging vs Boosting:

| Aspect          | Bagging                                     | Boosting                                          |
|-----------------|---------------------------------------------|---------------------------------------------------|
| Meaning         | Bootstrap Aggregating; multiple models trained in parallel | Sequentially trains models; each new model corrects errors from previous |
| Aim             | Reduces **variance** | Reduces **bias** |
| Model Training  | Independent training                        | Dependent training                                |
| Example Algorithms | Random Forest                             | AdaBoost, Gradient Boosting, XGBoost              |
| Data Sampling   | Random subsets (with replacement)           | Weighted data sampling based on previous errors    |

----------

### Brief Explanation:

-   **Bagging:**

    -   Each model gets a random subset of data and makes predictions independently.

    -   Final prediction is made by voting (classification) or averaging (regression).

-   **Boosting:**

    -   Models are built sequentially.

    -   Each new model focuses more on the incorrectly predicted examples by previous models.

    -   Final prediction is a weighted sum of predictions.

----------

✅ **Thus, ensemble learning is a very powerful technique for improving model performance.**

----------


# Question 3 (OR)

## (a)

**Explain single layer and multi-layer perceptron and neural networks with suitable example.**

### Answer:

### Neural Network Overview:

-   **Definition:**
    A Neural Network is a system of algorithms that tries to recognize underlying relationships in a set of data through a process that mimics how the human brain operates.

----------

### 1. Single Layer Perceptron:

-   **Definition:**
    A **single layer perceptron** consists of only **one layer** of output nodes connected directly to the input features.

-   **Working:**

    -   Takes multiple inputs, applies weights, sums them, passes through an activation function (like step function).

    -   Used for **linear classification problems** (i.e., data that is linearly separable
